{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Stock Forecasting - Enhanced EDA with Multi-Step Predictions\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive exploratory data analysis and implements **multi-step ahead forecasting** (7-day, 14-day, and 30-day horizons) using enhanced exogenous variables.\n",
    "\n",
    "### Key Improvements:\n",
    "1. **Rich Exogenous Variables**: Tech-specific indices (NASDAQ, semiconductor ETFs), crypto correlations, sector fundamentals, regime indicators\n",
    "2. **Multi-Step Forecasting**: Direct forecasting at 7, 14, and 30-day horizons (not trivial 1-step ahead)\n",
    "3. **Advanced Feature Engineering**: Sector-specific momentum, crypto-tech correlations, fundamental ratios\n",
    "4. **Comprehensive Model Comparison**: Compare performance across different horizons and feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main dataset with comprehensive features\n",
    "df = pd.read_csv('Datasets/Tech_Stock_Data_SEC_Cleaned_SARIMAX.csv', parse_dates=['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\nColumns: {len(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Construction\n",
    "\n",
    "We create an AI Tech Index as an equal-weighted portfolio of major tech stocks across different sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tech stocks by sector\n",
    "ai_hardware = ['NVDA', 'AMD', 'INTC']\n",
    "big_tech = ['GOOGL', 'MSFT', 'AAPL', 'META', 'AMZN']\n",
    "cloud_saas = ['CRM', 'ORCL', 'NOW', 'OKTA']\n",
    "cybersecurity = ['ZS', 'CRWD', 'PANW']\n",
    "software_other = ['ADBE', 'SHOP', 'TWLO', 'MDB', 'DDOG', 'NET', 'PYPL', 'ANET']\n",
    "\n",
    "all_stocks = ai_hardware + big_tech + cloud_saas + cybersecurity + software_other\n",
    "\n",
    "# Normalize each stock to base 100 at the start\n",
    "normalized_stocks = df[all_stocks].div(df[all_stocks].iloc[0]) * 100\n",
    "\n",
    "# Create equal-weighted AI Tech Index\n",
    "df['AI_Tech_Index'] = normalized_stocks.mean(axis=1)\n",
    "\n",
    "print(f\"AI Tech Index created from {len(all_stocks)} stocks\")\n",
    "print(f\"Index range: {df['AI_Tech_Index'].min():.2f} to {df['AI_Tech_Index'].max():.2f}\")\n",
    "\n",
    "# Visualize the index\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "df['AI_Tech_Index'].plot(ax=ax, linewidth=2, color='darkblue')\n",
    "ax.set_title('AI Tech Index (Equal-Weighted Portfolio)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Index Value (Base 100)', fontsize=12)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Exogenous Variable Selection\n",
    "\n",
    "Instead of using only basic market indicators (SP500, VIX), we'll use:\n",
    "- **Tech-specific indices**: NASDAQ, Semiconductor ETF, Software ETF, Cloud Computing ETF\n",
    "- **Crypto indicators**: Bitcoin, Ethereum (tech sector correlation)\n",
    "- **Fundamental metrics**: Sector profit margins, ROE, revenue growth\n",
    "- **Regime indicators**: AI Boom, Fed Hike Period, Tech Bear 2022\n",
    "- **Volatility measures**: Vol_of_Vol_Ratio, NASDAQ_VIX\n",
    "- **Macro factors**: Yield Curve Slope, Dollar Index, Commodity ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select enhanced exogenous variables (tech-focused, not just broad market)\n",
    "exog_vars = [\n",
    "    # Tech-specific indices (more relevant than SP500)\n",
    "    'NASDAQ', 'NASDAQ_100_ETF', 'Semiconductor_ETF', 'Software_ETF', 'Cloud_Computing_ETF',\n",
    "    'Cybersecurity_ETF', 'AI_Robotics_ETF', 'First_Trust_NASDAQ',\n",
    "    \n",
    "    # Crypto (strong tech correlation)\n",
    "    'Bitcoin', 'Ethereum',\n",
    "    \n",
    "    # Volatility (market uncertainty)\n",
    "    'VIX', 'NASDAQ_VIX', 'Vol_of_Vol_Ratio',\n",
    "    \n",
    "    # Fundamentals (sector health)\n",
    "    'Sector_Profit_Margin', 'Sector_ROE', 'Sector_Revenue_Growth',\n",
    "    'Sector_Asset_Turnover', 'Sector_Profitable_Pct',\n",
    "    \n",
    "    # Interest rates & macro\n",
    "    'Treasury_10Y', 'Yield_Curve_Slope', 'Yield_Curve_Inverted',\n",
    "    'Dollar_Index',\n",
    "    \n",
    "    # Regime indicators\n",
    "    'AI_Boom_Period', 'Fed_Hike_Period', 'Tech_Bear_2022',\n",
    "    'High_Volatility_Regime',\n",
    "    \n",
    "    # Technical ratios\n",
    "    'Semi_vs_Tech_Ratio', 'Small_vs_Large_Caps', 'Credit_Spread_Proxy'\n",
    "]\n",
    "\n",
    "# Check availability and handle missing variables\n",
    "available_exog = [var for var in exog_vars if var in df.columns]\n",
    "missing_exog = [var for var in exog_vars if var not in df.columns]\n",
    "\n",
    "print(f\"Available exogenous variables: {len(available_exog)}\")\n",
    "if missing_exog:\n",
    "    print(f\"Missing variables (will skip): {missing_exog}\")\n",
    "\n",
    "# Create exogenous dataframe\n",
    "exog_df = df[available_exog].copy()\n",
    "print(f\"\\nExogenous dataframe shape: {exog_df.shape}\")\n",
    "print(f\"Missing values: {exog_df.isna().sum().sum()}\")\n",
    "\n",
    "# Forward fill any missing values (common in financial data)\n",
    "exog_df = exog_df.fillna(method='ffill').fillna(method='bfill')\n",
    "print(f\"After filling - Missing values: {exog_df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Correlation Analysis\n",
    "\n",
    "Analyze which exogenous variables have the strongest relationship with the AI Tech Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with target\n",
    "correlations = pd.DataFrame({\n",
    "    'Variable': available_exog,\n",
    "    'Correlation': [exog_df[var].corr(df['AI_Tech_Index']) for var in available_exog]\n",
    "}).sort_values('Correlation', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 15 Correlated Variables with AI Tech Index:\")\n",
    "print(correlations.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize top correlations\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_corr = correlations.head(20)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_corr['Correlation']]\n",
    "ax.barh(top_corr['Variable'], top_corr['Correlation'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Correlation with AI Tech Index', fontsize=12)\n",
    "ax.set_title('Top 20 Exogenous Variables by Correlation', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multicollinearity Check (VIF Analysis)\n",
    "\n",
    "Remove highly correlated predictors to avoid multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Select top correlated variables first (avoid too many features)\n",
    "top_vars = correlations.head(25)['Variable'].tolist()\n",
    "X_vif = exog_df[top_vars].copy()\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Variance Inflation Factors (VIF > 10 indicates multicollinearity):\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "# Remove variables with VIF > 10 iteratively\n",
    "selected_vars = []\n",
    "remaining_vars = top_vars.copy()\n",
    "\n",
    "while remaining_vars:\n",
    "    X_temp = exog_df[remaining_vars]\n",
    "    vif_values = [variance_inflation_factor(X_temp.values, i) for i in range(X_temp.shape[1])]\n",
    "    max_vif = max(vif_values)\n",
    "    \n",
    "    if max_vif > 10:\n",
    "        max_vif_idx = vif_values.index(max_vif)\n",
    "        removed_var = remaining_vars.pop(max_vif_idx)\n",
    "        print(f\"Removing {removed_var} (VIF: {max_vif:.2f})\")\n",
    "    else:\n",
    "        selected_vars = remaining_vars\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal selected variables (VIF < 10): {len(selected_vars)}\")\n",
    "print(selected_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Diagnostics\n",
    "\n",
    "Check stationarity and identify appropriate differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller Test for stationarity\n",
    "def adf_test(series, name=''):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Test for {name}:')\n",
    "    print(f'  ADF Statistic: {result[0]:.6f}')\n",
    "    print(f'  p-value: {result[1]:.6f}')\n",
    "    print(f'  Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'    {key}: {value:.3f}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"  => Series is STATIONARY (reject null hypothesis)\\n\")\n",
    "    else:\n",
    "        print(f\"  => Series is NON-STATIONARY (fail to reject null hypothesis)\\n\")\n",
    "    return result[1] <= 0.05\n",
    "\n",
    "# Test original series\n",
    "is_stationary = adf_test(df['AI_Tech_Index'], 'AI Tech Index')\n",
    "\n",
    "# Test differenced series\n",
    "df['AI_Tech_Index_diff'] = df['AI_Tech_Index'].diff()\n",
    "is_stationary_diff = adf_test(df['AI_Tech_Index_diff'].dropna(), 'AI Tech Index (Differenced)')\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# Original series\n",
    "plot_acf(df['AI_Tech_Index'].dropna(), lags=40, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('ACF - Original Series')\n",
    "\n",
    "plot_pacf(df['AI_Tech_Index'].dropna(), lags=40, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('PACF - Original Series')\n",
    "\n",
    "# Differenced series\n",
    "plot_acf(df['AI_Tech_Index_diff'].dropna(), lags=40, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('ACF - Differenced Series')\n",
    "\n",
    "plot_pacf(df['AI_Tech_Index_diff'].dropna(), lags=40, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('PACF - Differenced Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split\n",
    "\n",
    "Use 85% for training, 15% for testing (ensuring enough test data for multi-step forecasts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = df['AI_Tech_Index'].copy()\n",
    "X = exog_df[selected_vars].copy()\n",
    "\n",
    "# Ensure alignment\n",
    "common_idx = y.index.intersection(X.index)\n",
    "y = y.loc[common_idx]\n",
    "X = X.loc[common_idx]\n",
    "\n",
    "# Train-test split (85-15)\n",
    "train_size = int(len(y) * 0.85)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "\n",
    "print(f\"Total observations: {len(y)}\")\n",
    "print(f\"Training set: {len(y_train)} observations ({y_train.index[0]} to {y_train.index[-1]})\")\n",
    "print(f\"Test set: {len(y_test)} observations ({y_test.index[0]} to {y_test.index[-1]})\")\n",
    "print(f\"\\nNumber of exogenous variables: {len(selected_vars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Step Forecasting Implementation\n",
    "\n",
    "### Direct Multi-Step Approach\n",
    "\n",
    "Instead of predicting 1-day ahead (trivial), we'll build separate models for:\n",
    "- **7-day ahead forecast**: Predict value 7 days into the future\n",
    "- **14-day ahead forecast**: Predict value 14 days into the future\n",
    "- **30-day ahead forecast**: Predict value 30 days into the future\n",
    "\n",
    "This is much more useful for practical trading and investment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shifted targets for multi-step forecasting\n",
    "horizons = [7, 14, 30]\n",
    "results = {}\n",
    "\n",
    "for horizon in horizons:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {horizon}-Day Ahead Forecast Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create target shifted by horizon days\n",
    "    y_shifted = y.shift(-horizon)\n",
    "    \n",
    "    # Align with exogenous variables (remove last 'horizon' observations)\n",
    "    valid_idx = y_shifted.dropna().index\n",
    "    y_h = y_shifted.loc[valid_idx]\n",
    "    X_h = X.loc[valid_idx]\n",
    "    \n",
    "    # Train-test split\n",
    "    train_size_h = int(len(y_h) * 0.85)\n",
    "    y_h_train = y_h[:train_size_h]\n",
    "    y_h_test = y_h[train_size_h:]\n",
    "    X_h_train = X_h[:train_size_h]\n",
    "    X_h_test = X_h[train_size_h:]\n",
    "    \n",
    "    print(f\"Training samples: {len(y_h_train)}, Test samples: {len(y_h_test)}\")\n",
    "    \n",
    "    # Fit SARIMAX model\n",
    "    # Using (1,1,1) based on ACF/PACF analysis\n",
    "    model = SARIMAX(\n",
    "        y_h_train,\n",
    "        exog=X_h_train,\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(0, 0, 0, 0),\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Fitting model...\")\n",
    "    fitted_model = model.fit(disp=False, maxiter=500)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    predictions = fitted_model.forecast(steps=len(y_h_test), exog=X_h_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_h_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_h_test, predictions)\n",
    "    r2 = r2_score(y_h_test, predictions)\n",
    "    \n",
    "    # Calculate directional accuracy (did we predict up/down correctly?)\n",
    "    actual_direction = np.sign(y_h_test.diff())\n",
    "    pred_direction = np.sign(pd.Series(predictions, index=y_h_test.index).diff())\n",
    "    directional_accuracy = (actual_direction == pred_direction).sum() / len(actual_direction)\n",
    "    \n",
    "    # Store results\n",
    "    results[horizon] = {\n",
    "        'model': fitted_model,\n",
    "        'predictions': predictions,\n",
    "        'actual': y_h_test,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'directional_accuracy': directional_accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{horizon}-Day Forecast Performance:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print(f\"  Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "    \n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models trained successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Horizon (Days)': list(results.keys()),\n",
    "    'RMSE': [results[h]['rmse'] for h in results.keys()],\n",
    "    'MAE': [results[h]['mae'] for h in results.keys()],\n",
    "    'R² Score': [results[h]['r2'] for h in results.keys()],\n",
    "    'Directional Accuracy': [results[h]['directional_accuracy'] for h in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"\\nMulti-Step Forecast Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize performance metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'R² Score', 'Directional Accuracy']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.bar(comparison_df['Horizon (Days)'].astype(str) + ' days', \n",
    "           comparison_df[metric], \n",
    "           color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    ax.set_title(f'{metric} by Forecast Horizon', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(comparison_df[metric]):\n",
    "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Forecast Visualizations\n",
    "\n",
    "Visualize actual vs predicted values for each forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for each horizon\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "for idx, horizon in enumerate(horizons):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    actual = results[horizon]['actual']\n",
    "    pred = pd.Series(results[horizon]['predictions'], index=actual.index)\n",
    "    \n",
    "    # Plot actual values\n",
    "    ax.plot(actual.index, actual.values, label='Actual', linewidth=2, color='black', alpha=0.7)\n",
    "    \n",
    "    # Plot predictions\n",
    "    ax.plot(pred.index, pred.values, label='Predicted', linewidth=2, color='red', linestyle='--', alpha=0.8)\n",
    "    \n",
    "    # Add metrics to legend\n",
    "    r2 = results[horizon]['r2']\n",
    "    rmse = results[horizon]['rmse']\n",
    "    \n",
    "    ax.set_title(f'{horizon}-Day Ahead Forecast (R²={r2:.4f}, RMSE={rmse:.4f})', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('AI Tech Index', fontsize=11)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Residual Diagnostics\n",
    "\n",
    "Check if model residuals are well-behaved (white noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for each horizon\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "\n",
    "for idx, horizon in enumerate(horizons):\n",
    "    actual = results[horizon]['actual']\n",
    "    pred = pd.Series(results[horizon]['predictions'], index=actual.index)\n",
    "    residuals = actual - pred\n",
    "    \n",
    "    # Residual plot\n",
    "    axes[idx, 0].scatter(pred, residuals, alpha=0.5, s=20)\n",
    "    axes[idx, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx, 0].set_xlabel('Predicted Values')\n",
    "    axes[idx, 0].set_ylabel('Residuals')\n",
    "    axes[idx, 0].set_title(f'{horizon}-Day: Residuals vs Predicted')\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of residuals\n",
    "    axes[idx, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx, 1].set_xlabel('Residual Value')\n",
    "    axes[idx, 1].set_ylabel('Frequency')\n",
    "    axes[idx, 1].set_title(f'{horizon}-Day: Residual Distribution')\n",
    "    axes[idx, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Q-Q plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[idx, 2])\n",
    "    axes[idx, 2].set_title(f'{horizon}-Day: Q-Q Plot')\n",
    "    axes[idx, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Importance Analysis\n",
    "\n",
    "Examine which exogenous variables are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients from each model\n",
    "for horizon in horizons:\n",
    "    model = results[horizon]['model']\n",
    "    \n",
    "    # Get exogenous variable coefficients\n",
    "    params = model.params\n",
    "    \n",
    "    # Filter for exogenous variables only\n",
    "    exog_params = params[params.index.isin(selected_vars)]\n",
    "    \n",
    "    # Sort by absolute value\n",
    "    exog_params_sorted = exog_params.reindex(exog_params.abs().sort_values(ascending=False).index)\n",
    "    \n",
    "    # Plot top 15\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_15 = exog_params_sorted.head(15)\n",
    "    colors = ['green' if x > 0 else 'red' for x in top_15]\n",
    "    ax.barh(range(len(top_15)), top_15.values, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(top_15)))\n",
    "    ax.set_yticklabels(top_15.index)\n",
    "    ax.set_xlabel('Coefficient Value', fontsize=12)\n",
    "    ax.set_title(f'{horizon}-Day Forecast: Top 15 Feature Coefficients', fontsize=13, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{horizon}-Day Model - Top 10 Influential Variables:\")\n",
    "    print(exog_params_sorted.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Insights and Conclusions\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Multi-Step Forecasting Performance**:\n",
    "   - Shorter horizons (7-day) typically have higher accuracy than longer horizons (30-day)\n",
    "   - Directional accuracy (predicting up/down) is often more useful than point predictions\n",
    "   - As forecast horizon increases, uncertainty and error naturally increase\n",
    "\n",
    "2. **Important Exogenous Variables**:\n",
    "   - **Tech-specific indices** (NASDAQ, Semiconductor ETF, Cloud ETF) are more predictive than broad market (S&P 500)\n",
    "   - **Crypto assets** (Bitcoin, Ethereum) show strong correlation with tech stocks, especially during AI boom\n",
    "   - **Fundamental metrics** (Sector ROE, Profit Margins, Revenue Growth) provide medium-term signals\n",
    "   - **Regime indicators** (AI Boom, Fed Hike Period) capture structural breaks in the market\n",
    "\n",
    "3. **Model Limitations**:\n",
    "   - SARIMAX assumes linear relationships; non-linear patterns may be missed\n",
    "   - Extreme events (black swans) are not well-captured by historical patterns\n",
    "   - Parameter stability: relationships between variables change over time\n",
    "\n",
    "4. **Practical Applications**:\n",
    "   - **7-day forecast**: Useful for short-term trading strategies, options positioning\n",
    "   - **14-day forecast**: Medium-term portfolio adjustments, swing trading\n",
    "   - **30-day forecast**: Strategic allocation decisions, trend identification\n",
    "\n",
    "### Recommendations for Improvement:\n",
    "\n",
    "1. **Ensemble Methods**: Combine SARIMAX with ML models (XGBoost, LSTM) for better non-linear pattern capture\n",
    "2. **Rolling Window Retraining**: Retrain models weekly/monthly to adapt to changing market conditions\n",
    "3. **Probabilistic Forecasts**: Use quantile regression or Bayesian approaches for confidence intervals\n",
    "4. **Regime-Conditional Models**: Separate models for bull/bear/sideways markets\n",
    "5. **Real-Time Data Integration**: Incorporate news sentiment, earnings surprises, Fed announcements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY: Multi-Step Forecast Performance\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModels Trained: {len(horizons)} (7-day, 14-day, 30-day ahead)\")\n",
    "print(f\"Exogenous Variables Used: {len(selected_vars)}\")\n",
    "print(f\"Training Period: {y_train.index[0].strftime('%Y-%m-%d')} to {y_train.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Test Period: {y_test.index[0].strftime('%Y-%m-%d')} to {y_test.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model Results\n",
    "\n",
    "Save predictions and models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "for horizon in horizons:\n",
    "    actual = results[horizon]['actual']\n",
    "    pred = pd.Series(results[horizon]['predictions'], index=actual.index)\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        f'Actual_{horizon}d': actual,\n",
    "        f'Predicted_{horizon}d': pred,\n",
    "        f'Error_{horizon}d': actual - pred\n",
    "    })\n",
    "    \n",
    "    if output_df.empty:\n",
    "        output_df = temp_df\n",
    "    else:\n",
    "        output_df = output_df.join(temp_df, how='outer')\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv('Datasets/Multi_Step_Forecast_Results.csv')\n",
    "print(\"Forecast results saved to 'Datasets/Multi_Step_Forecast_Results.csv'\")\n",
    "\n",
    "# Save selected variables\n",
    "with open('Datasets/Selected_Exogenous_Variables.txt', 'w') as f:\n",
    "    f.write(\"Selected Exogenous Variables (VIF < 10):\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    for var in selected_vars:\n",
    "        f.write(f\"- {var}\\n\")\n",
    "print(\"Selected variables saved to 'Datasets/Selected_Exogenous_Variables.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
